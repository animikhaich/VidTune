{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ani/miniconda3/envs/mg/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from audiocraft.models import musicgen\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import json\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "GOOGLE_API_KEY = 'AIzaSyALlIi2z90_mOq8Ac-Yy3ZK_dOxp2ps_L8'\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file_name = \"videos/3A49B385FD4A8FE2E3AEEF43C140D9AF_video_dashinit.mp4\"\n",
    "mllm_model = genai.GenerativeModel('gemini-1.5-flash-latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a music supervisor who analyzes the content and tone of images and videos to describe music that fits well with the mood, evokes emotions, and enhances the narrative of the visuals. Given an image or video, describe the scene and generate a prompt suitable for music generation models. Use keywords related to genre, instruments, mood, context, and setting to craft a concise single-sentence prompt, like:\n",
      "\n",
      "\n",
      "'A dynamic blend of hip-hop and orchestral elements, with sweeping strings and brass, evoking the vibrant energy of the city',\n",
      "'Smooth jazz, with a saxophone solo, piano chords, and snare full drums',\n",
      "'90s rock song with electric guitar and heavy drums'.\n",
      "\n",
      "\n",
      "You must return your response using this JSON schema: \n",
      "{\"Content Description\": \"string\", \"Music Prompt\": \"string\"}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "music_prompt_examples = '''\n",
    "'A dynamic blend of hip-hop and orchestral elements, with sweeping strings and brass, evoking the vibrant energy of the city',\n",
    "'Smooth jazz, with a saxophone solo, piano chords, and snare full drums',\n",
    "'90s rock song with electric guitar and heavy drums'.\n",
    "'''\n",
    "\n",
    "json_schema = '''\n",
    "{\"Content Description\": \"string\", \"Music Prompt\": \"string\"}\n",
    "'''\n",
    "\n",
    "gemni_instructions = f'''\n",
    "You are a music supervisor who analyzes the content and tone of images and videos to describe music that fits well with the mood, evokes emotions, and enhances the narrative of the visuals. Given an image or video, describe the scene and generate a prompt suitable for music generation models. Use keywords related to genre, instruments, mood, context, and setting to craft a concise single-sentence prompt, like:\n",
    "\n",
    "{music_prompt_examples}\n",
    "\n",
    "You must return your response using this JSON schema: {json_schema}\n",
    "'''\n",
    "\n",
    "print(gemni_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".Using music prompt: A gentle instrumental piece with a focus on acoustic guitar, soft piano chords, and subtle percussion, creating a serene and meditative atmosphere that complements the slow and deliberate drawing process.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m generated_prompt \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(generated_prompt\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```json\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     42\u001b[0m description, music_prompt \u001b[38;5;241m=\u001b[39m generated_prompt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent Description\u001b[39m\u001b[38;5;124m\"\u001b[39m], generated_prompt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMusic Prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 44\u001b[0m \u001b[43mgenerate_music\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmusic_prompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mgenerate_music\u001b[0;34m(music_prompt)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_music\u001b[39m(music_prompt):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing music prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmusic_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmusicgen_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmusic_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMusic generated!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.9/site-packages/audiocraft/models/genmodel.py:161\u001b[0m, in \u001b[0;36mBaseGenModel.generate\u001b[0;34m(self, descriptions, progress, return_tokens)\u001b[0m\n\u001b[1;32m    159\u001b[0m attributes, prompt_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_tokens_and_attributes(descriptions, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m prompt_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tokens:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_audio(tokens), tokens\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.9/site-packages/audiocraft/models/musicgen.py:256\u001b[0m, in \u001b[0;36mMusicGen._generate_tokens\u001b[0;34m(self, attributes, prompt_tokens, progress)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduration \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_duration:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# generate by sampling from LM, simple case.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast:\n\u001b[0;32m--> 256\u001b[0m         gen_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_gen_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# now this gets a bit messier, we need to handle prompts,\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# melody conditioning etc.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     ref_wavs \u001b[38;5;241m=\u001b[39m [attr\u001b[38;5;241m.\u001b[39mwav[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_wav\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m attributes]\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.9/site-packages/audiocraft/models/lm.py:510\u001b[0m, in \u001b[0;36mLMModel.generate\u001b[0;34m(self, prompt, conditions, num_samples, max_gen_len, use_sampling, temp, top_k, top_p, cfg_coef, two_step_cfg, remove_prompts, check, callback, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (curr_sequence \u001b[38;5;241m==\u001b[39m unknown_token)\u001b[38;5;241m.\u001b[39many()\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# sample next token from the model, next token shape is [B, K, 1]\u001b[39;00m\n\u001b[0;32m--> 510\u001b[0m next_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_next_token\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurr_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg_conditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munconditional_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_sampling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_coef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtwo_step_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtwo_step_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m# ensure the tokens that should be masked are properly set to special_token_id\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# as the model never output special_token_id\u001b[39;00m\n\u001b[1;32m    515\u001b[0m valid_mask \u001b[38;5;241m=\u001b[39m mask[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, offset:offset\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mexpand(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.9/site-packages/audiocraft/models/lm.py:387\u001b[0m, in \u001b[0;36mLMModel._sample_next_token\u001b[0;34m(self, sequence, cfg_conditions, unconditional_state, use_sampling, temp, top_k, top_p, cfg_coef, two_step_cfg)\u001b[0m\n\u001b[1;32m    385\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39msample_top_p(probs, p\u001b[38;5;241m=\u001b[39mtop_p)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m top_k \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 387\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_top_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    389\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mmultinomial(probs, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mg/lib/python3.9/site-packages/audiocraft/utils/utils.py:119\u001b[0m, in \u001b[0;36msample_top_k\u001b[0;34m(probs, k)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sample next token from top K values along the last dimension of the input probs tensor.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Sampled tokens.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m top_k_value, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(probs, k, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m min_value_top_k \u001b[38;5;241m=\u001b[39m \u001b[43mtop_k_value\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    120\u001b[0m probs \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m (probs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_value_top_k)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    121\u001b[0m probs\u001b[38;5;241m.\u001b[39mdiv_(probs\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "musicgen_model = musicgen.MusicGen.get_pretrained('facebook/musicgen-small',device='cuda')\n",
    "musicgen_model.set_generation_params(duration=30)\n",
    "\n",
    "def process_video_in_gemini(video_file_name):\n",
    "    video_file = genai.upload_file(video_file_name)\n",
    "    while video_file.state.name == \"PROCESSING\":\n",
    "        print(\".\",end=\"\")\n",
    "        time.sleep(3)\n",
    "        video_file = genai.get_file(video_file.name)\n",
    "\n",
    "    if video_file.state.name == \"FAILED\":\n",
    "        raise ValueError(video_file.state.name)\n",
    "    \n",
    "    response = mllm_model.generate_content(\n",
    "        [video_file, 'Explain what is happening in this video'],\n",
    "        request_options={\"timeout\":600}\n",
    "    )\n",
    "\n",
    "    cleaned_response = mllm_model.generate_content([\n",
    "        response.text,\n",
    "        gemni_instructions\n",
    "    ])\n",
    "\n",
    "    return cleaned_response.text\n",
    "\n",
    "def generate_music(music_prompt):\n",
    "    print(f\"Using music prompt: {music_prompt}\")\n",
    "    result = musicgen_model.generate(music_prompt, progress=False)\n",
    "    print(f\"Music generated!\")\n",
    "    result = result.squeeze().cpu().numpy().T\n",
    "    sample_rate = musicgen_model.sample_rate\n",
    "    print(f\"Sample rate: {sample_rate}\")\n",
    "    with open(\"output.wav\", \"wb\") as f:\n",
    "        f.write(result)\n",
    "    print(f\"Music saved to {output_file}\")\n",
    "    return music\n",
    "\n",
    "\n",
    "generated_prompt = process_video_in_gemini(video_file_name)\n",
    "generated_prompt = json.loads(generated_prompt.strip(\"```json\\n\"))\n",
    "\n",
    "description, music_prompt = generated_prompt[\"Content Description\"], generated_prompt[\"Music Prompt\"]\n",
    "\n",
    "generate_music(music_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
